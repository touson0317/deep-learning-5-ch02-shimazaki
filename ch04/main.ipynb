{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ad08af",
   "metadata": {},
   "source": [
    "# 第4章 word2vecの高速化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a82a3",
   "metadata": {},
   "source": [
    "## 4.1 word2vecの改良①"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955fb44",
   "metadata": {},
   "source": [
    "### 4.1.1 Embeddingレイヤ\n",
    "シンプルなCBOWモデルでは実装に問題がある．\n",
    "例えば，単語数が100万語あるとき，$W_{in}$，$W_{out}$，softmaxの計算ばボトルネックになる．\n",
    "そこで，Embeddingレイヤを導入する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12b46a0",
   "metadata": {},
   "source": [
    "### 4.1.2 Embeddingレイヤの実装\n",
    "matmalレイヤでは，列ベクトルと行列の演算を行っているが，意味としては，行列の単に行列の特定の行を抽出しているだけである．\n",
    "\n",
    "単語IDに該当する行を抜き出すためのレイヤーを作成する．\n",
    "* embeddingは単語の埋め込みという用語に由来する．\n",
    "* Embeddingレイヤに単語の埋め込み，分散表現が格納される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07797129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../deep-learning-from-scratch-2')\n",
    "from common.config import GPU\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        if GPU:\n",
    "            import cupy as cp\n",
    "            cp.scatter_add(dW, self.idx, dout)\n",
    "        else:\n",
    "            np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb43f0",
   "metadata": {},
   "source": [
    "## 4.2 word2vecの改良②\n",
    "中間層以降の処理\n",
    "ネガティブサンプリングを行う"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3e48c",
   "metadata": {},
   "source": [
    "### 4.2.1 中間層以降の計算の問題点\n",
    "中間層と重みの積\n",
    "softmaxレイヤの計算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2cdde0",
   "metadata": {},
   "source": [
    "### 4.2.2 多値分類から二値分類へ\n",
    "多値分類を２値分類で近似するアイデア\n",
    "\n",
    "これまでやってきたことは，コンテキストから正解となる単語を高い確率で推測できるようにすること．\n",
    "\n",
    "これは多値分類だが，これを２値分類に変更する．\n",
    "* コンテキストがyouとgoodbyeとするときにターゲットとなる単語はsayですかという質問に変える．\n",
    "\n",
    "出力層のニューロンは一つだけで，中間層と出力側の重みの行列の積において，sayに対応する列だけを抽出すれば良い．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9cca3",
   "metadata": {},
   "source": [
    "### 4.2.3 シグモイド関数と交差エントロピー誤差\n",
    "多値分類においては，softmax関数と交差エントロピー誤差を用いる．２知分類では，sigmoid関数と交差エントロピー誤差を用いる．sigmoid関数は以下の式である．\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "交差エントロピー誤差は以下の式である．\n",
    "$$L = -(t \\log y + (1 -t) \\log (1-y)) $$\n",
    "tは正解ラベル．tが1の時は，正解がyes，tが0の時は，正解がno．\n",
    "tが1の時は，-logyが出力される．tが0の時は，-log(1-y)が出力される．\n",
    "逆伝播において，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5501e2",
   "metadata": {},
   "source": [
    "### 4.2.4 多値分類から二値分類へ（実装編）\n",
    "$W_{out}$においてもembeddingの機能を持たせる．\n",
    "\n",
    "embedはembeddingレイヤをparamにはパラメータをgradsには勾配を格納する．cacheは順伝播の際に必要な情報を保持する．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07e61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf1d331",
   "metadata": {},
   "source": [
    "### 4.2.5 Negative Sampling\n",
    "負の例の方は対象となっていない．\n",
    "そのため，ネガティブな例を少数サンプリングしてLossを計算する\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11c3b6",
   "metadata": {},
   "source": [
    "### 4.2.6 Negative Samplingのサンプリング手法\n",
    "\n",
    "コーパスの統計データを用いて，使われやすい単語は抽出されやすくし，あまり使われないデータは抽出されないようにする．\n",
    "各ちる分布に対して，一手間加えた値を用いて，サンプリングを行う．\n",
    "$$\n",
    "P^{'}(w_i) = \\frac{P( w_i)^{3/4}}{\\sum_j P(w_j)^{3/4}}\n",
    "$$\n",
    "\n",
    "出現確率が低い単語を見捨てないようにするため\n",
    "\n",
    "\n",
    "UnigramSampler\n",
    "クラスで実装する．\n",
    "ユニグラムは，一つの単語を意味する．\n",
    "バイグラムは，２つの連続した単語を意味する．例えば，(you, say)や(you goodbye)などの２つの単語の組み合わせを対象とした確率分布を作るということ．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f1b7d",
   "metadata": {},
   "source": [
    "### 4.2.7 Negative Samplingの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da535db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        if not GPU:\n",
    "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                p = self.word_p.copy()\n",
    "                target_idx = target[i]\n",
    "                p[target_idx] = 0\n",
    "                p /= p.sum()\n",
    "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "        else:\n",
    "            # GPU(cupy）で計算するときは、速度を優先\n",
    "            # 負例にターゲットが含まれるケースがある\n",
    "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "                                               replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample\n",
    "\n",
    "\n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "\n",
    "        # 正例のフォワード\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "\n",
    "        # 負例のフォワード\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f6e4d",
   "metadata": {},
   "source": [
    "## 4.3 改良版word2vecの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb7eeb8",
   "metadata": {},
   "source": [
    "### 4.3.1 CBOWモデルの実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db89ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)  # Embeddingレイヤを使用\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b597ce8",
   "metadata": {},
   "source": [
    "## 4.4 word2vecに関する残りのテーマ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf332f0",
   "metadata": {},
   "source": [
    "### 4.4.1 word2vecを使ったアプリケーションの例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d6b66",
   "metadata": {},
   "source": [
    "類似単語を求める用途に利用できる．\n",
    "しかし，単語の分散表現が重要な理由は転移学習(transfer learning)にある．自然言語のタスクを解く際に，単語の分散表現をゼロから学習することはほとんどない．そうではなく，先に大きなコーパスで学習を行い，その学習済みの分散表現を個別のタスクで利用する．例えば，テキスト分類や文書クラスタりんぐ，品詞タグ付け，感情分析などの自然言語のタスクにおいて，単語をベクトルに変換する最初のステップでは，学習済みの単語の分散表現を利用することができる．その多種多様な自然言語処理のタスクのほとんど全てにおいて，単語の分散表現は素晴らしい結果をもたらす．\n",
    "\n",
    "また，単語の分散表現の利点は，単語を固定長のベクトルに変換できることにある．さらに，文章（単語の並び）に対しても，単語の分散表現を使って固定長のベクトルに変換できる．RNNを用いることによって文章を固定長のベクトルに変換できる\n",
    "\n",
    "単語を固定長のベクトルに変換できることによって，一般的な機械学習の手法が適用できるから非常に重要である．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2081ba9",
   "metadata": {},
   "source": [
    "### 4.4.2 単語ベクトルの評価方法\n",
    "\n",
    "単語の分散表現を得ることができるが，これの良さはどのように評価するべきか．\n",
    "\n",
    "現実的には，何かしらのアプリケーションにおいて，単語の分散表現が使われる際に，最終的に望まれるのは精度の良いシステムである．ここで考えなければいけないことは，システムが複数のシステムから構成されるということ．例えば，単語の分散表現を作るシステムと特定の問題に対して分類を行うシステムがある．この場合は，２段階の学習を行なった上で，評価する必要がある．しかし，これには２つのシステムにおいてハイパーパラメータのためのチューニングが必要であり，多くの時間がかかってしまう．\n",
    "\n",
    "そこで，単語の分散表現の評価を行うにあたり，現実的なアプリケーションとは切り離して評価を行うというのが一般的によく行われる．その際によく用いられる評価指標が類似性や類推問題である．\n",
    "\n",
    "単語類似性の評価では，人間が作成した単語類似度の評価セットを使って評価することが多く行われる．例えば，catとanimalの類似度は0.8，catとcarの類似度は0.2などのように，人間が評価した単語の類似度を用いて，単語ベクトルの類似度を評価する．\n",
    "\n",
    "類推問題による評価は，king:queen = man: ?のような類推問題を出題し，その正解率でもって単語の分散表現の良さを図る．類推問題によって，単語の意味や文法的な問題を正しく理解しているかということをある程度計測することができる．そのため，類推問題を精度良く解くことのできる単語の分散表現であれば，自然言語を扱うアプリケーションにおいても良い結果が期待できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4b5e27",
   "metadata": {},
   "source": [
    "## 4.5 まとめ\n",
    "* Embeddingレイヤは単語の分散表現を格納し，順伝播において，該当する単語のIDのベクトルを抽出する．\n",
    "* word2vecでは，語彙数の増加に比例して計算量が増加するので，近似計算を行う高速な手法を使うと良い\n",
    "* Negative Samplingは負の例をいくつかサンプリングする手法であり，これを利用すれば多値分類を2値分類として扱うことができる．\n",
    "* word2vecで得られた単語の分散表現は，単語の意味が埋め込まれたものであり，似たコンテキストで使われる単語は単語のベクトルの空間として近い場所に位置するようになる\n",
    "* word2vecの単語の分散表現は類推問題をベクトルの加算と減算によって解ける性質を持つ\n",
    "* word2vecの単語の分散表現は類推問題をベクトルの加算と減算によって解ける性質を持つ\n",
    "* word2vecは転移学習の点で特に重要であり，その単語の分散表現はさまざまな自然言語処理のタスクに利用できる．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2eb60",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
