{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8c867c",
   "metadata": {},
   "source": [
    "# 第2章 自然言語と単語の分散表現"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61651c90",
   "metadata": {},
   "source": [
    "## 2.1 自然言語処理とは"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b939b88",
   "metadata": {},
   "source": [
    "### 2.1.1 単語の意味\n",
    "\n",
    "自然言語をコンピュータに理解させるためには，単語の意味を理解させることが重要である．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2809b",
   "metadata": {},
   "source": [
    "## 2.2 シソーラス\n",
    "\n",
    "自然言語の歴史を振り返ると，単語の意味を人手で定義する試みが行われてきた．\n",
    "ただし，広辞苑のように人が使う一般的な辞書ではなく，シソーラス（thesaurus）と呼ばれるタイプの辞書である．\n",
    "シソーラスとは\n",
    "* 類義語辞書\n",
    "* 単語間で，上位と下位，全体と部分などの細かい関連性\n",
    "* 単語間の関係をグラフで表す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973ab69",
   "metadata": {},
   "source": [
    "### 2.2.1 WordNet\n",
    "1985年に開発がスタート．\n",
    "プリンストン大学で開発されたシソーラスである．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d032da",
   "metadata": {},
   "source": [
    "### 2.2.2 シソーラスの問題点\n",
    "時代の変化に対応するのが困難\n",
    "* 時とともに新しい単語が生まれ，古い単語は忘れられる\n",
    "* 時代によって言葉の意味が変化することもある\n",
    "人の作業コストが高い\n",
    "* 現存する英単語の総数は1000万語以上\n",
    "* Wordnetですら20万後程度\n",
    "単語の細かなニュアンスを表現できない\n",
    "\n",
    "人手で定義する手法の代わりにカウントベースの手法を次節で紹介する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5fa80e",
   "metadata": {},
   "source": [
    "## 2.3 カウントベースの手法\n",
    "\n",
    "カウントベース手法において，コーパスを使う\n",
    "* コーパス(corpus)とは，言語の使用例を集めたデータセット\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac114e",
   "metadata": {},
   "source": [
    "### 2.3.1 Pythonによるコーパスの下準備\n",
    "pythonを用いてコーパスを準備する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1631eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: [0 1 2 3 4 1 5 6]\n",
      "Word to ID: {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "ID to Word: {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(\"Corpus:\", corpus)\n",
    "print(\"Word to ID:\", word_to_id)\n",
    "print(\"ID to Word:\", id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6de0df",
   "metadata": {},
   "source": [
    "### 2.3.2 単語の分散表現\n",
    "色には固有の名前がつけられている（コバルトブルー，シングレッド）\n",
    "一方で，RGBのように色を数値で表現することもできる．\n",
    "ここで注目したい点は，RGBのようなベクトル表現の方が正確に色を指定することができるという点である．\n",
    "同様に，単語の意味をベクトルで表現することができないか？\n",
    "\n",
    "これから目指すべき場所は，単語の意味を的確に捉えたベクトル表現である．\n",
    "このようなベクトル表現を単語の分散表現と呼ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73098da4",
   "metadata": {},
   "source": [
    "### 2.3.3 分布仮説\n",
    "分布仮説(distributional hypothesis)とは\n",
    "* 単語の意味は，周囲の単語によって形成されるというもの\n",
    "\n",
    "コンテキストとは\n",
    "* ある中央の単語に対して，その周囲にある単語を指す\n",
    "* コンテキストのサイズをwindow sizeと呼ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abe654",
   "metadata": {},
   "source": [
    "### 2.3.4 共起行列\n",
    "分布仮説に基づいて，単語をベクトルで表す方法を考える．\n",
    "そのための素直な方法は，周囲の単語をカウントすることである．\n",
    "* カウントベースの手法を統計的手法ということもある．\n",
    "\n",
    "すべての単語に対して，コンテキストとして共起する単語をテーブルにまとめたものを共起行列(co-occurrence matrix)と呼ぶ．\n",
    "\n",
    "以下にpythonでの実装例を示す．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ca8b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence Matrix:\n",
      " [[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''共起行列の作成\n",
    "\n",
    "    :param corpus: コーパス（単語IDのリスト）\n",
    "    :param vocab_size:語彙数\n",
    "    :param window_size:ウィンドウサイズ（ウィンドウサイズが1のときは、単語の左右1単語がコンテキスト）\n",
    "    :return: 共起行列\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "co_matrix = create_co_matrix(corpus, vocab_size, window_size=1)\n",
    "print(\"Co-occurrence Matrix:\\n\", co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1734c",
   "metadata": {},
   "source": [
    "### 2.3.5 ベクトル間の類似度\n",
    "ベクトル間の類似度を計算する方法として，コサイン類似度(cosine similarity)がある．\n",
    "コサイン類似度は$\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$と$\\mathbf{y} = (y_1, y_2, \\ldots, y_n)$の間の角度を計算するもので，次のように定義される．\n",
    "\n",
    "$$\n",
    "\\text{similarity}(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{||\\mathbf{x}|| \\cdot ||\\mathbf{y}||} = \\frac{x_1 y_1 + x_2 y_2 + \\ldots + x_n y_n}{\\sqrt{x_1^2 + x_2^2 + \\ldots + x_n^2} \\cdot \\sqrt{y_1^2 + y_2^2 + \\ldots + y_n^2}}\n",
    "$$\n",
    "\n",
    "ここで，$\\mathbf{x} \\cdot \\mathbf{y}$は内積，$||\\mathbf{x}||$はベクトルのノルム（大きさ）である．\n",
    "実装は以下のようになる．\n",
    "\n",
    "```python\n",
    "def cos_similarity(x, y):\n",
    "    nx = x / np.sqrt(np.sum(x ** 2)) #xを正規化\n",
    "    ny = y / np.sqrt(np.sum(y ** 2)) #yを正規化\n",
    "    return np.dot(nx, ny)\n",
    "```\n",
    "\n",
    "この実装には問題がある．それはゼロベクトルが含まれている場合に，ゼロ除算が発生する可能性があることである．\n",
    "これに対する常套手段は除算をする際に小さな値を足すことである．\n",
    "以下のように修正する．\n",
    "```python\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps) #xを正規化\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps) #yを正規化\n",
    "    return np.dot(nx, ny)\n",
    "```\n",
    "\n",
    "この関数を用いると，単語ベクトルの類似度は次のように表される．ここではyouとIの類似度を計算する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3d6a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''コサイン類似度の算出\n",
    "\n",
    "    :param x: ベクトル\n",
    "    :param y: ベクトル\n",
    "    :param eps: ”0割り”防止のための微小値\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]  #「you」の単語ベクトル\n",
    "c1 = C[word_to_id['i']]  #「i」の単語ベクトル\n",
    "print(cos_similarity(c0, c1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e63e4",
   "metadata": {},
   "source": [
    "### 2.3.6 類似単語のランキング表示\n",
    "\n",
    "コサイン類似度の実装が完了したので，その関数を用いて別の便利な関数を実装する．それは，ある単語がクエリとして与えられたときにそのクエリに対して類似した単語を上位から順に表示する関数である．\n",
    "以下のようにmost_similar関数を実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9e2fe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n",
      "\n",
      "[query] i\n",
      " goodbye: 0.9999999858578643\n",
      " you: 0.7071067691154799\n",
      " hello: 0.49999999292893216\n",
      " say: 0.0\n",
      " and: 0.0\n",
      "\n",
      "Argsortの例:\n",
      "Original array: [300 100  40 200]\n",
      "sorted indices: [2 1 3 0]\n",
      "sorted indices (descending): [0 3 1 2]\n"
     ]
    }
   ],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''類似単語の検索\n",
    "\n",
    "    :param query: クエリ（テキスト）\n",
    "    :param word_to_id: 単語から単語IDへのディクショナリ\n",
    "    :param id_to_word: 単語IDから単語へのディクショナリ\n",
    "    :param word_matrix: 単語ベクトルをまとめた行列。各行に対応する単語のベクトルが格納されていることを想定する\n",
    "    :param top: 上位何位まで表示するか\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "most_similar('you', word_to_id, id_to_word, C, top=5)\n",
    "most_similar('i', word_to_id, id_to_word, C, top=5)\n",
    "\n",
    "# argsortの使い方を実践\n",
    "print(\"\\nArgsortの例:\")\n",
    "x = np.array([300, 100,40, 200])\n",
    "print(\"Original array:\", x)\n",
    "print(\"sorted indices:\", x.argsort())\n",
    "print(\"sorted indices (descending):\", (-x).argsort())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cfb99",
   "metadata": {},
   "source": [
    "上の結果では，youとIの類似度が高いのは，人称代名詞だから納得できる．しかし，goodbyeやhelloのコサイン類似度の値が高いのは感覚と大きなズレがある．この理由はコーパスが小さいからである．\n",
    "次節では，カウントベースの手法の改善について説明する．\n",
    "\n",
    "## 2.4 カウントベースの手法の改善"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ec80c",
   "metadata": {},
   "source": [
    "### 2.4.1 相互情報量\n",
    "これまでの共起行列において，以下の問題がある．\n",
    "* theとcarの単語の共起を考える．\n",
    "* the carというフレーズは多く見られると予想される．\n",
    "* carとdriveという単語には明らかに強い関連性がある．\n",
    "* しかし，単に出現回数だけを考えると，carとdriveの関連性はtheとcarの関連性よりも低くなる可能性がある．\n",
    "* これは，theのような頻出単語が存在するためである．\n",
    "この問題を解決するために，相互情報量(point wise mutual information, PMI)を用いる．\n",
    "相互情報量は次のように定義される．\n",
    "$$\\text{PMI}(x, y) = \\log _2 \\frac{P(x, y)}{P(x) P(y)}$$\n",
    "ここで，$P(x, y) $は単語xとyが同時に出現する確率，$P(x)$は単語xが出現する確率，$P(y)$は単語yが出現する確率である．\n",
    "例えば，10000この単語からなるコーパスで，theという単語が100回出現したとする．\n",
    "このとき，$P(\\text{the}) = 100 / 10000 = 0.01$となる．\n",
    "theとcarが10回共起したとすると，$P(\\text{the,car}) = 10 / 10000 = 0.001$となる．\n",
    "先ほどのPMIの式を変形する．共起行列を$C$とする，\n",
    "$$\\text{PMI}(x, y) = \\log _2 \\frac{\\frac{C(x, y)}{N}}{\\frac{C(x)}{N} \\cdot \\frac{C(y)}{N}} = \\log _2 \\frac{C(x, y) \\times N}{C(x) C(y)}$$\n",
    "ここで，$C(x, y)$は単語xとyの共起回数，$C(x)$は単語xの出現回数，$C(y)$は単語yの出現回数である．\n",
    "この式を用いて，PMIを計算することができる．しかし，２つの単語で共起する回数が0の時，logが定義できない，それに対応するため，正の相互情報量(positive PMI, PPMI)を用いる．\n",
    "$$\\text{PPMI}(x, y) = \\max(0, \\text{PMI}(x, y))$$\n",
    "\n",
    "単語xとyの共起する回数を$C(x, y)$とし，単語xの出現回数を$C(x)$，単語yの出現回数を$C(y)$とした時に，$C(x) = \\Sigma _ i C(i, x)$，$C(y) = \\Sigma _ i C(i, y)$，$N = \\Sigma _ i  \\Sigma _ j C(i, j)$とする．実装例を下に示す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8430bfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_id: {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "co-occurrence matrix\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ppmi(C, verbose=False, eps = 1e-8):\n",
    "    '''PPMI（正の相互情報量）の作成\n",
    "\n",
    "    :param C: 共起行列\n",
    "    :param verbose: 進行状況を出力するかどうか\n",
    "    :return:\n",
    "    '''\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100 + 1) == 0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "    return M\n",
    "\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "print(\"word_to_id:\", word_to_id)\n",
    "np.set_printoptions(precision=3)  # 有効桁３桁で表示\n",
    "print('co-occurrence matrix')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934f3d3",
   "metadata": {},
   "source": [
    "しかし，PPMI行列にも問題がある．\n",
    "* PPMI行列は疎な行列である．ーつまり，ほとんどの要素は重要ではない\n",
    "* 語彙数が増えると，PPMI行列のサイズは指数的に増加する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca060442",
   "metadata": {},
   "source": [
    "### 2.4.2 次元削減\n",
    "特異値分解を用いて次元削減を行う．\n",
    "数式を下に書く\n",
    "$$C = U \\Sigma V^T$$\n",
    "ここで，$C$はPPMI行列，$U$は左特異ベクトル，$\\Sigma$は特異値，$V$は右特異ベクトルである．\n",
    "$U$は直行行列で，単語空間を表す．\n",
    "$\\Sigma$は対角行列で，特異値を含む．\n",
    "$V$は直行行列で，コンテキスト空間を表す\n",
    "特異値が小さいほど，重要度が低いので，行列$U$から特異値が小さい列を削除することで次元削減を行う．\n",
    "行列$C$の各行には，対応する単語IDの単語ベクトルが格納されており，それらの単語ベクトルが行列$U'$の行に格納される．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73cf98",
   "metadata": {},
   "source": [
    "### 2.4.3 SVDによる次元削減\n",
    "pythonで実装を行う．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f27db3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence matrix:\n",
      " [0 1 0 0 0 0 0]\n",
      "PPMI matrix:\n",
      " [0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "Left singular vectors (U):\n",
      " [-1.110e-16  3.409e-01 -1.205e-01 -3.886e-16  0.000e+00 -9.323e-01\n",
      "  8.768e-17]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(id_to_word)\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=1)\n",
    "W = ppmi(C)\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "np.set_printoptions(precision=3)  # 有効桁３桁で表示\n",
    "print(\"Co-occurrence matrix:\\n\", C[0])\n",
    "print(\"PPMI matrix:\\n\", W[0])\n",
    "print(\"Left singular vectors (U):\\n\", U[0])\n",
    "\n",
    "# plot\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c88c050",
   "metadata": {},
   "source": [
    "### 2.4.4 PTBデータセット\n",
    "本格的なコーパスを利用する．それはPenn Treebank（PTB）データセットである．\n",
    "元となるPTBに対して，いくつかの前処理を行う．例えば，レアな単語をunk(unknown)と置き換えたり，具体的な数字をN(num)で置き換えされたりしている．PTBコーパスでは，一つの文が一行ごとに保存されており，各文を連結したものを「一つの大きな時系列データ」として扱う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b683f688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ptb.train.txt ... \n",
      "Done\n",
      "Done\n",
      "corpus size: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "word_to_id['lexus']: 7426\n",
      "corpus size: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "word_to_id['lexus']: 7426\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../deep-learning-from-scratch-2')\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "print('corpus size:', len(corpus))\n",
    "print('corpus[:30]:', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print()\n",
    "print(\"word_to_id['car']:\", word_to_id['car'])\n",
    "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
    "print(\"word_to_id['lexus']:\", word_to_id['lexus'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49d182",
   "metadata": {},
   "source": [
    "### 2.4.5 PTBデータセットでの評価\n",
    "sklearnのrandomized_svdを用いて，PTBデータセットの単語ベクトルを計算する．特異値の大きいものだけに限定して計算することで，通常のsvdよりも高速に計算を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7814b305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting  co-occurrence ...\n",
      "calculating PPMI ...\n",
      "calculating PPMI ...\n",
      "1.0% done\n",
      "1.0% done\n",
      "2.0% done\n",
      "2.0% done\n",
      "3.0% done\n",
      "3.0% done\n",
      "4.0% done\n",
      "4.0% done\n",
      "5.0% done\n",
      "5.0% done\n",
      "6.0% done\n",
      "6.0% done\n",
      "7.0% done\n",
      "7.0% done\n",
      "8.0% done\n",
      "8.0% done\n",
      "9.0% done\n",
      "9.0% done\n",
      "10.0% done\n",
      "10.0% done\n",
      "11.0% done\n",
      "11.0% done\n",
      "12.0% done\n",
      "12.0% done\n",
      "13.0% done\n",
      "13.0% done\n",
      "14.0% done\n",
      "14.0% done\n",
      "15.0% done\n",
      "15.0% done\n",
      "16.0% done\n",
      "16.0% done\n",
      "17.0% done\n",
      "17.0% done\n",
      "18.0% done\n",
      "18.0% done\n",
      "19.0% done\n",
      "19.0% done\n",
      "20.0% done\n",
      "20.0% done\n",
      "21.0% done\n",
      "21.0% done\n",
      "22.0% done\n",
      "22.0% done\n",
      "23.0% done\n",
      "23.0% done\n",
      "24.0% done\n",
      "24.0% done\n",
      "25.0% done\n",
      "25.0% done\n",
      "26.0% done\n",
      "26.0% done\n",
      "27.0% done\n",
      "27.0% done\n",
      "28.0% done\n",
      "28.0% done\n",
      "29.0% done\n",
      "29.0% done\n",
      "30.0% done\n",
      "30.0% done\n",
      "31.0% done\n",
      "31.0% done\n",
      "32.0% done\n",
      "32.0% done\n",
      "33.0% done\n",
      "33.0% done\n",
      "34.0% done\n",
      "34.0% done\n",
      "35.0% done\n",
      "35.0% done\n",
      "36.0% done\n",
      "36.0% done\n",
      "37.0% done\n",
      "37.0% done\n",
      "38.0% done\n",
      "38.0% done\n",
      "39.0% done\n",
      "39.0% done\n",
      "40.0% done\n",
      "40.0% done\n",
      "41.0% done\n",
      "41.0% done\n",
      "42.0% done\n",
      "42.0% done\n",
      "43.0% done\n",
      "43.0% done\n",
      "44.0% done\n",
      "44.0% done\n",
      "45.0% done\n",
      "45.0% done\n",
      "46.0% done\n",
      "46.0% done\n",
      "47.0% done\n",
      "47.0% done\n",
      "48.0% done\n",
      "48.0% done\n",
      "49.0% done\n",
      "49.0% done\n",
      "50.0% done\n",
      "50.0% done\n",
      "51.0% done\n",
      "51.0% done\n",
      "52.0% done\n",
      "52.0% done\n",
      "53.0% done\n",
      "53.0% done\n",
      "54.0% done\n",
      "54.0% done\n",
      "55.0% done\n",
      "55.0% done\n",
      "56.0% done\n",
      "56.0% done\n",
      "57.0% done\n",
      "57.0% done\n",
      "58.0% done\n",
      "58.0% done\n",
      "59.0% done\n",
      "59.0% done\n",
      "60.0% done\n",
      "60.0% done\n",
      "61.0% done\n",
      "61.0% done\n",
      "62.0% done\n",
      "62.0% done\n",
      "63.0% done\n",
      "63.0% done\n",
      "64.0% done\n",
      "64.0% done\n",
      "65.0% done\n",
      "65.0% done\n",
      "66.0% done\n",
      "66.0% done\n",
      "67.0% done\n",
      "67.0% done\n",
      "68.0% done\n",
      "68.0% done\n",
      "69.0% done\n",
      "69.0% done\n",
      "70.0% done\n",
      "70.0% done\n",
      "71.0% done\n",
      "71.0% done\n",
      "72.0% done\n",
      "72.0% done\n",
      "73.0% done\n",
      "73.0% done\n",
      "74.0% done\n",
      "74.0% done\n",
      "75.0% done\n",
      "75.0% done\n",
      "76.0% done\n",
      "76.0% done\n",
      "77.0% done\n",
      "77.0% done\n",
      "78.0% done\n",
      "78.0% done\n",
      "79.0% done\n",
      "79.0% done\n",
      "80.0% done\n",
      "80.0% done\n",
      "81.0% done\n",
      "81.0% done\n",
      "82.0% done\n",
      "82.0% done\n",
      "83.0% done\n",
      "83.0% done\n",
      "84.0% done\n",
      "84.0% done\n",
      "85.0% done\n",
      "85.0% done\n",
      "86.0% done\n",
      "86.0% done\n",
      "87.0% done\n",
      "87.0% done\n",
      "88.0% done\n",
      "88.0% done\n",
      "89.0% done\n",
      "89.0% done\n",
      "90.0% done\n",
      "90.0% done\n",
      "91.0% done\n",
      "91.0% done\n",
      "92.0% done\n",
      "92.0% done\n",
      "93.0% done\n",
      "93.0% done\n",
      "94.0% done\n",
      "94.0% done\n",
      "95.0% done\n",
      "95.0% done\n",
      "96.0% done\n",
      "96.0% done\n",
      "97.0% done\n",
      "97.0% done\n",
      "98.0% done\n",
      "98.0% done\n",
      "99.0% done\n",
      "99.0% done\n",
      "calculating SVD ...\n",
      "calculating SVD ...\n",
      "\n",
      "[query] you\n",
      "\n",
      "[query] you\n",
      " i: 0.6970505118370056\n",
      " we: 0.6350072622299194\n",
      " 'll: 0.5433908700942993\n",
      " do: 0.542751669883728\n",
      " 'd: 0.5378462076187134\n",
      "\n",
      "[query] year\n",
      " earlier: 0.6622394323348999\n",
      " month: 0.590505838394165\n",
      " next: 0.5876365900039673\n",
      " quarter: 0.5677922368049622\n",
      " last: 0.5505343079566956\n",
      "\n",
      "[query] car\n",
      " luxury: 0.6222271919250488\n",
      " auto: 0.5368207693099976\n",
      " truck: 0.4873467683792114\n",
      " cars: 0.48062533140182495\n",
      " domestic: 0.47767242789268494\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7308551669120789\n",
      " motors: 0.6586288213729858\n",
      " lexus: 0.5917172431945801\n",
      " honda: 0.5488848686218262\n",
      " mazda: 0.5424246788024902\n",
      " i: 0.6970505118370056\n",
      " we: 0.6350072622299194\n",
      " 'll: 0.5433908700942993\n",
      " do: 0.542751669883728\n",
      " 'd: 0.5378462076187134\n",
      "\n",
      "[query] year\n",
      " earlier: 0.6622394323348999\n",
      " month: 0.590505838394165\n",
      " next: 0.5876365900039673\n",
      " quarter: 0.5677922368049622\n",
      " last: 0.5505343079566956\n",
      "\n",
      "[query] car\n",
      " luxury: 0.6222271919250488\n",
      " auto: 0.5368207693099976\n",
      " truck: 0.4873467683792114\n",
      " cars: 0.48062533140182495\n",
      " domestic: 0.47767242789268494\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7308551669120789\n",
      " motors: 0.6586288213729858\n",
      " lexus: 0.5917172431945801\n",
      " honda: 0.5488848686218262\n",
      " mazda: 0.5424246788024902\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('counting  co-occurrence ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('calculating PPMI ...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('calculating SVD ...')\n",
    "try:\n",
    "    # truncated SVD (fast!)\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                             random_state=None)\n",
    "except ImportError:\n",
    "    # SVD (slow)\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a411c1",
   "metadata": {},
   "source": [
    "## 2.5 まとめ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
